{"cells":[{"cell_type":"markdown","metadata":{"id":"x2hTo2N8Jl5I"},"source":["# 1. NUCLEI SEGMENTATION - STARDIST (PYTHON)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNlr6ucDkfrV"},"outputs":[],"source":["from stardist.data import test_image_nuclei_2d\n","from stardist.plot import render_label\n","from stardist.models import StarDist2D\n","from shapely.geometry import Polygon, Point\n","import matplotlib.image\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import matplotlib.patches as patches\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import PIL\n","import tifffile\n","import sys\n","import pickle\n","#import scanpy as sc\n","#import os\n","#import bin2cell as b2c\n","#import cv2\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOaNCeyalDN5"},"outputs":[],"source":["# functions\n","\n","def custom_normalize(x, pmin=3, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n","    \"\"\"Percentile-based image normalization.\"\"\"\n","\n","    mi = np.array([[[134.]]])#np.percentile(x,pmin,axis=axis,keepdims=True)\n","    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n","    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n","\n","\n","def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):\n","    if dtype is not None:\n","        x   = x.astype(dtype,copy=False)\n","        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n","        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n","        eps = dtype(eps)\n","\n","    try:\n","        import numexpr\n","        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n","    except ImportError:\n","        x =                   (x - mi) / ( ma - mi + eps )\n","\n","    if clip:\n","        x = np.clip(x,0,1)\n","\n","    return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TfVDgPEmIQwX"},"outputs":[],"source":["# INPUT DATA\n","\n","# img hires\n","PIL.Image.MAX_IMAGE_PIXELS = 1718032108\n","img = tifffile.imread(source_image_path)\n","img = img[:,:,0:3]\n","\n","# Histological_image from sample.csv\n","# dare in input dal csv dei sample il valore di Histological_image in variabile hist_img to choose StarDist model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ax9XNevJIQ_b"},"outputs":[],"source":["# normalizzazione immagine custom\n","\n","normalized_img = custom_normalize(img)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPGVOXBQIRD2"},"outputs":[],"source":["# choose stardist model based on image, hist_img vedi input\n","\n","if hist_img == \"H&E\":\n","    model_stardist = \"2D_versatile_he\"\n","elif hist_img == \"IF\":\n","    model_stardist = \"2D_versatile_fluo\"\n","\n","model = StarDist2D.from_pretrained(model_stardist)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pw8t8UF0IRHo"},"outputs":[],"source":["# run stardist model\n","\n","labels, polys = model.predict_instances_big(\n","    normalized_img, axes='YXC', block_size=4096, min_overlap=128, context=128,\n","    normalizer=None, # n_tiles=(4,4,1),\n","    prob_thresh = .05)\n","\n","# potenzialmente prob_thresh e nms_thresh (qui default) potrebbero essere parametri da dare in input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ishnSAiSIRKl"},"outputs":[],"source":["# save labels and polys\n","#with open(results_folder + sample_name + '_nuclei_labels.pkl', 'wb') as f:  # open a text file\n","#    pickle.dump(labels, f)\n","# i labels non dovrebbero servire più\n","\n","with open(results_folder + sample_name + '_nuclei_polys.pkl', 'wb') as f:  # open a text file\n","    pickle.dump(polys, f)"]},{"cell_type":"markdown","metadata":{"id":"R63KcW9rKtC3"},"source":["# 2. BIN2CELL EXPANSION (PYTHON)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THmgrwYknFTN"},"outputs":[],"source":["from stardist import random_label_cmap, _draw_polygons\n","import pandas as pd\n","import numpy as np\n","import scanpy as sc\n","from scipy import sparse\n","import bin2cell as b2c\n","from stardist.models import StarDist2D\n","from csbdeep.utils import normalize\n","import cv2\n","import os\n","import pickle\n","import anndata\n","import geopandas as gpd\n","from tifffile import imread, imwrite\n","from stardist.models import StarDist2D\n","from shapely.geometry import Polygon, Point\n","from shapely.affinity import scale\n","from scipy.spatial.distance import pdist\n","import scrublet as scr\n","import itertools\n","import anndata as ad\n","from pandas.api.types import CategoricalDtype\n","import random\n","import sys\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"66nRkaWvol8k"},"outputs":[],"source":["# functions\n","\n","def remove_destripe_artifacts(adata):\n","    idx = adata.X.sum(axis=1).A1 == float('inf')\n","    print('removing '+str(idx.sum())+' dots')\n","    return adata[idx==False]\n","\n","def nuclei_detection(polys, tissue_position_file, adata):\n","    # Creating a list to store Polygon geometries\n","    geometries = []\n","\n","    # Iterating through each nuclei in the 'polys' DataFrame\n","    for nuclei in range(len(polys['coord'])):\n","\n","        # Extracting coordinates for the current nuclei and converting them to (y, x) format\n","        coords = [(y, x) for x, y in zip(polys['coord'][nuclei][0], polys['coord'][nuclei][1])]\n","\n","        # Creating a Polygon geometry from the coordinates\n","        geometries.append(Polygon(coords))\n","\n","    # Creating a GeoDataFrame using the Polygon geometries\n","    gdf = gpd.GeoDataFrame(geometry=geometries)\n","    gdf['id'] = [f\"ID_{i+1}\" for i, _ in enumerate(gdf.index)]\n","\n","    ########################################################################\n","\n","    # Load the Spatial Coordinates\n","    df_tissue_positions=pd.read_parquet(tissue_position_file)\n","\n","    #Set the index of the dataframe to the barcodes\n","    df_tissue_positions = df_tissue_positions.set_index('barcode')\n","\n","    # Create an index in the dataframe to check joins\n","    df_tissue_positions['index']=df_tissue_positions.index\n","\n","    # Adding the tissue positions to the meta data\n","    adata.obs =  pd.merge(adata.obs, df_tissue_positions[['pxl_row_in_fullres', 'pxl_col_in_fullres']], left_index=True, right_index=True)\n","\n","    # Create a GeoDataFrame from the DataFrame of coordinates\n","    geometry = [Point(xy) for xy in zip(df_tissue_positions['pxl_col_in_fullres'], df_tissue_positions['pxl_row_in_fullres'])]\n","    gdf_coordinates = gpd.GeoDataFrame(df_tissue_positions, geometry=geometry)\n","\n","    #########################################################################\n","\n","    # Perform a spatial join to check which coordinates are in a cell nucleus\n","    result_spatial_join = gpd.sjoin(gdf_coordinates, gdf, how='left', predicate='within')\n","\n","    result_spatial_join[\"in_tissue\"] = np.array(result_spatial_join[\"in_tissue\"]).astype(bool)\n","\n","    # Identify nuclei associated barcodes and find barcodes that are in more than one nucleus\n","    result_spatial_join['is_within_polygon'] = ~result_spatial_join['index_right'].isna()\n","    barcodes_in_overlaping_polygons = pd.unique(result_spatial_join[result_spatial_join.duplicated(subset=['index'])]['index'])\n","    result_spatial_join['is_not_in_an_polygon_overlap'] = ~result_spatial_join['index'].isin(barcodes_in_overlaping_polygons)\n","\n","    # Remove barcodes in overlapping nuclei\n","    barcodes_in_one_polygon =  result_spatial_join[result_spatial_join['is_not_in_an_polygon_overlap'] & result_spatial_join[\"in_tissue\"]] # result_spatial_join['is_within_polygon'] &\n","    # tengo anche quelli unassigned to any nucleus perchè potrebbero essere in seguito aggregati come citoplasma\n","    # tanto in bin2cell per l'aggregazione dei nuclei gli 0 non verranno aggregati: Integers, with 0 being unassigned to an object.\n","\n","    # The AnnData object is filtered to only contain the barcodes that are in non-overlapping polygon regions\n","    filtered_obs_mask = adata.obs_names.isin(barcodes_in_one_polygon['index'])\n","    filtered_adata = adata[filtered_obs_mask,:]\n","\n","    # Add the results of the point spatial join to the Anndata object\n","    filtered_adata.obs =  pd.merge(filtered_adata.obs, barcodes_in_one_polygon[['index','geometry','id','is_within_polygon','is_not_in_an_polygon_overlap']], left_index=True, right_index=True)\n","\n","    filtered_adata.obs.id = filtered_adata.obs.id.str.replace('ID_', '', regex=False)\n","    # filtered_adata.obs = filtered_adata.obs.fillna(0)\n","    filtered_adata.obs.id = filtered_adata.obs.id.fillna(0)\n","    filtered_adata.obs.id = filtered_adata.obs.id.astype(str)\n","    print(f\"Number of nuclei detected of hires image: {gdf.shape[0]}\\nNumber of nuclei detected on VisiumHD slide: {len(np.unique(filtered_adata.obs.id))}\")\n","\n","    # filter and returns also the geometry dataframe associated to nuclei\n","    gdf['id'] = gdf['id'].str.replace('ID_', '', regex=False)\n","    gdf = gdf.loc[gdf['id'].isin(filtered_adata.obs.id)]\n","    gdf.set_index('id', inplace=True)\n","\n","    return filtered_adata, gdf\n","\n","\n","def add_obs_variables(adata, unit, species='Hs'):\n","    x = adata.copy()\n","    x.obs['counts_per_' + unit] = np.sum(x.X, axis = 1)\n","    x.obs['features_per_' + unit] = np.sum(x.X >0, axis = 1)\n","\n","    if \"bin_count\" in adata.obs.columns:\n","        x.obs[\"bin_count_log\"] = np.log10(x.obs[\"bin_count\"])\n","    x.obs[\"counts_per_\" + unit + \"_log\"] = np.log10(x.obs[\"counts_per_\" + unit])\n","    x.obs[\"features_per_\" + unit + \"_log\"] = np.log10(x.obs[\"features_per_\" + unit])\n","\n","    if species=='Hs':\n","        x.var[\"mt\"] = x.var_names.str.startswith(\"MT-\")\n","    if species=='Mm':\n","        x.var[\"mt\"] = x.var_names.str.startswith(\"mt-\")\n","    sc.pp.calculate_qc_metrics(\n","        x, qc_vars=[\"mt\"], percent_top=None, log1p=False, inplace=True\n","    )\n","    x.obs.loc[np.isnan(x.obs.pct_counts_mt),'pct_counts_mt'] = 0\n","    return x\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z95jAN4EojQ7"},"outputs":[],"source":["# INPUT DATA\n","\n","# polys\n","#with open(results_folder + sample_name + '_nuclei_labels.pkl', 'rb') as f:  # open a text file\n","#    labels = pickle.load(f)\n","with open(results_folder + sample_name + '_nuclei_polys.pkl', 'rb') as f:  # open a text file\n","    polys = pickle.load(f)\n","\n","# adata\n","adata = sc.read_h5ad(output_adata_path)\n","\n","# parquet\n","tissue_position_file = # path_name di binned_outputs/square_002um/spatial/tissue_positions.parquet\n","\n","# Species dal sample.csv in variabile species"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUUJbecvLX_8"},"outputs":[],"source":["# preprocessing adata\n","\n","sc.pp.filter_genes(adata, min_cells=3)  # ok filtro sui geni\n","sc.pp.filter_cells(adata, min_counts=0) # non filtro i pixel perchè andranno aggregati\n","b2c.destripe(adata,adjust_counts=True)\n","adata = remove_destripe_artifacts(adata)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vur69tkaLYGe"},"outputs":[],"source":["# add segmentation labels to h5ad object, map stardist modeled nuclei on the adata\n","\n","adata, gdf = nuclei_detection(\n","    polys = polys, adata=adata,\n","    tissue_position_file = tissue_position_file\n","    )\n","adata.obs.id = adata.obs.id.astype(int)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuxMlOPJLYK1"},"outputs":[],"source":["# group nuclei based on labels \"id\"\n","\n","nuclei_grouped = b2c.bin_to_cell(adata, labels_key=\"id\", spatial_keys=[\"spatial\"])\n","\n","# annotate\n","nuclei_grouped = add_obs_variables(nuclei_grouped, \"nucleus\", species=species)\n","nuclei_grouped.obs[\"id\"] = np.array(nuclei_grouped.obs.index)\n","nuclei_grouped.obs['geometry'] = gdf.loc[nuclei_grouped.obs.id]['geometry']\n","nuclei_grouped.obs['max_diameter'] = [max_diameter(x) for x in nuclei_grouped.obs['geometry']]\n","nuclei_grouped.obs['zero_mt'] = nuclei_grouped.obs.pct_counts_mt == 0\n","print('Using ' + str(np.round(nuclei_grouped.X.sum() / adata.X.sum() * 100)) + '% of the total read counts')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Tj3IOooqdk8"},"outputs":[],"source":["# remove empty nuclei and point geometries\n","\n","idx = nuclei_grouped.obs.features_per_nucleus >= 2\n","print('Number of nuclei with at least 2 features: ' + str(np.sum(idx)))\n","nuclei_grouped = nuclei_grouped[idx]\n","idx = [geom.geom_type  == 'Polygon' for geom in nuclei_grouped.obs['geometry']]\n","print('Number of nuclei with polygon geometry: ' + str(np.sum(idx)))\n","nuclei_grouped = nuclei_grouped[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxZ_S3pAqdKF"},"outputs":[],"source":["# expand nuclei into cells\n","\n","# reset to \"0\" (i.e. not assigned) pixels assigned to non-valid nulclei\n","adata.obs.loc[~adata.obs.id.isin(nuclei_grouped.obs.index.astype(int)),'id'] = 0\n","\n","# expand nuclei\n","#adata.obs.id = adata.obs.id.astype(int)   # to expand needs to be numeric\n","expand = 2      # 2 bins = 4 microns, forse conviene metterlo come parametro in input per fare diversi tentativi di espansione\n","expansion_label = \"id_exp_\"+str(expand)\n","b2c.expand_labels(adata,\n","                  labels_key='id',\n","                  expanded_labels_key=expansion_label,\n","                  max_bin_distance = expand\n","                 )\n","adata.obs[expansion_label] = adata.obs[expansion_label].astype(int)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BsyWxuAN3Pf"},"outputs":[],"source":["# group cells based on labels\n","expanded_nuclei = b2c.bin_to_cell(adata, labels_key=expansion_label, spatial_keys=[\"spatial\"])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epc7zCWNsUGJ"},"outputs":[],"source":["# define new geometries and filter out cells (and corresponding nuclei) with non-Polygon geometry\n","cell_labels = set(adata.obs[expansion_label])\n","cell_labels.remove(0)\n","cell_geoms = {x : cell_geometry(adata[adata.obs[expansion_label] == x].obs['geometry']) for x in cell_labels}\n","idx = [geom.geom_type  == 'Polygon' for geom in cell_geoms.values()]\n","print('Number of cells with polygon geometry: ' + str(np.sum(idx)))\n","nuclei_grouped = nuclei_grouped[idx]\n","expanded_nuclei = expanded_nuclei[idx]\n","cell_geoms = {int(x) : cell_geoms[int(x)] for x in expanded_nuclei.obs_names}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbcwyOaFsRmz"},"outputs":[],"source":["# annotate cells\n","expanded_nuclei = add_obs_variables(expanded_nuclei, \"cell\", species=species)\n","expanded_nuclei.obs[\"id\"] = np.array(expanded_nuclei.obs.index)\n","expanded_nuclei.obs['geometry'] = [cell_geoms[int(x)] for x in expanded_nuclei.obs.index]\n","expanded_nuclei.obs['max_diameter'] = [max_diameter(x) for x in expanded_nuclei.obs['geometry']]\n","expanded_nuclei.obs['zero_mt'] = expanded_nuclei.obs.pct_counts_mt == 0\n","print('Using ' + str(np.round(expanded_nuclei.X.sum() / adata.X.sum() * 100)) + '% of the total read counts')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xh-DRBOSs6u8"},"outputs":[],"source":["# save geometry on disk\n","nuclei_grouped_geometry = nuclei_grouped.obs['geometry']\n","expanded_nuclei_geometry = expanded_nuclei.obs['geometry']\n","with open(results_folder+'/'+sample_name+'_nuclei_grouped_geometry.pkl', 'wb') as f:  # open a text file,\n","    pickle.dump(nuclei_grouped_geometry, f),\n","with open(results_folder+'/'+sample_name+'_expanded_nuclei_geometry.pkl', 'wb') as f:  # open a text file,\n","    pickle.dump(expanded_nuclei_geometry, f)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfTWQnwFOfVj"},"outputs":[],"source":["# remove geometries and save h5ad on disk of nuclei and cells\n","del nuclei_grouped.obs['geometry']\n","del expanded_nuclei.obs['geometry']\n","nuclei_grouped.write_h5ad(results_folder+'/'+sample_name+'_nuclei_grouped.h5ad')\n","expanded_nuclei.write_h5ad(results_folder+'/'+sample_name+'_expanded_nuclei.h5ad')\n"]},{"cell_type":"markdown","metadata":{"id":"uAfuOy8_tLmG"},"source":["# 3. RCTD (R) - vedi script"]},{"cell_type":"markdown","metadata":{"id":"QTa5VAiwtROE"},"source":["# 4. FILTERS ON DECONVOLUTION RESULTS (R) - vedi script parte 2"]},{"cell_type":"markdown","metadata":{"id":"Zj0EHorx0RJt"},"source":["# 5. SELECT CELLS/NUCLEI AND DEFINE NEW ADATA (PYTHON)\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344},"executionInfo":{"elapsed":6,"status":"error","timestamp":1744802656020,"user":{"displayName":"Gaia Gaia","userId":"13107295718462017787"},"user_tz":-120},"id":"LC-4IDUhBRXd","outputId":"1ea41ddf-04f7-4854-d262-9016053b5c0c"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'scanpy'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-81690ce05754>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# pip install --user scanpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscanpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scanpy'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import pandas as pd\n","import scanpy as sc\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bH8Fso30hYf"},"outputs":[],"source":["# INPUT\n","df_class_RCTD = pd.read_csv(path + 'RCTD_filters.csv')   # RCTD results\n","\n","# NUCLEI GROUPED\n","nuclei_grouped = sc.read_h5ad(results_folder+'/'+sample_name+'_nuclei_grouped.h5ad')\n","\n","# EXPANDED NUCLEI\n","expanded_nuclei = sc.read_h5ad(results_folder+'/'+sample_name+'_expanded_nuclei.h5ad')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuEGKt4CBbC-"},"outputs":[],"source":["# select nuclei or cells\n","nuclei_grouped.obs_names = nuclei_grouped.obs_names.astype(str)\n","nuclei_selected = (np.array(df_class_RCTD.index[df_class_RCTD['type'] == 'nuclei'])).astype(str)\n","nuclei_grouped_filtered = nuclei_grouped[nuclei_selected, :]\n","nuclei_grouped_filtered = nuclei_grouped_filtered.copy()\n","nuclei_grouped_filtered.obs[\"cell_nucleus\"] = \"nucleus\"\n","\n","expanded_nuclei.obs_names = expanded_nuclei.obs_names.astype(str)\n","cells_selected = (np.array(df_class_RCTD.index[df_class_RCTD['type'] == 'cells'])).astype(str)\n","expanded_nuclei_filtered = expanded_nuclei[cells_selected, :]\n","expanded_nuclei_filtered = expanded_nuclei_filtered.copy()\n","expanded_nuclei_filtered.obs[\"cell_nucleus\"] = \"cell\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VkZa_x46Dcty"},"outputs":[],"source":["# merge nuclei and cells in adata_final\n","adata_final = sc.concat([nuclei_grouped_filtered, expanded_nuclei_filtered], join='outer')\n","adata_final.obs[\"cell_types\"] = df_class_RCTD.loc[adata_final.obs.index, \"class\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abkWfkDiHZHP"},"outputs":[],"source":["# final object on which to perform downstream analyses\n","adata_final.write_h5ad(results_folder + sample_name+'_adata_final.h5ad')"]},{"cell_type":"markdown","metadata":{"id":"tG6sHLJ-tje6"},"source":["# 6. STANDARD CLUSTERING ANALYSIS (PYTHON)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dlDL2bntqkI"},"outputs":[],"source":["import matplotlib.image\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import matplotlib.patches as patches\n","from matplotlib.colors import ListedColormap\n","from stardist import random_label_cmap, _draw_polygons\n","import pandas as pd\n","import numpy as np\n","import scanpy as sc\n","from scipy import sparse\n","import bin2cell as b2c\n","from stardist.models import StarDist2D\n","from csbdeep.utils import normalize\n","import cv2\n","import os\n","import pickle\n","import anndata\n","import geopandas as gpd\n","from tifffile import imread, imwrite\n","from stardist.models import StarDist2D\n","from shapely.geometry import Polygon, Point\n","import seaborn as sns\n","import scrublet as scr\n","#pip install celltypist\n","import celltypist\n","from celltypist import models\n","import itertools\n","import anndata as ad\n","from pandas.api.types import CategoricalDtype"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ZyLuB_Ktql5"},"outputs":[],"source":["def up_to_pca(adata, n_top_genes=3000, from_raw=True):\n","    print('Top genes: '+str(n_top_genes))\n","    if from_raw:\n","        x = adata.raw.to_adata()\n","    else:\n","        x = adata.copy()\n","        sc.pp.normalize_total(x)\n","        sc.pp.log1p(x)\n","    if n_top_genes == 0:\n","        sc.pp.highly_variable_genes(x, min_mean=0.0125, max_mean=3, min_disp=-2)\n","    else:\n","        sc.pp.highly_variable_genes(x, n_top_genes=n_top_genes)\n","    sc.pl.highly_variable_genes(x)\n","    x.raw = x\n","    x = x[:, x.var.highly_variable]\n","    sc.pp.scale(x, max_value = 10)\n","    sc.tl.pca(x)\n","    sc.pl.pca_variance_ratio(x, n_pcs=50)\n","    return x\n","\n","def from_pca_to_leiden(adata, n_neighbors = 20, n_pcs = 15, resolution = 1, plot_spatial=True):\n","    print('Neighbors: '+str(n_neighbors))\n","    print('PCA n: '+str(n_pcs))\n","    print('resolution: '+str(resolution))\n","    x = adata.copy()\n","    sc.pp.neighbors(x, n_neighbors = n_neighbors, n_pcs = n_pcs)\n","    sc.tl.umap(x)\n","    sc.tl.leiden(x, flavor=\"igraph\", resolution= resolution)\n","    n_clusters = len(np.unique(x.obs.leiden))\n","    x.uns[\"leiden_colors\"] = create_palette_matplotlib(len(x.obs.leiden.cat.categories), white=False)\n","    sc.pl.umap(x, color=[\"leiden\"], legend_loc = 'on data')\n","    if plot_spatial:\n","    \tsc.pl.spatial(x, color=\"leiden\")\n","    return x\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CIw9B3dQuuma"},"outputs":[],"source":["# INPUT DATA\n","\n","# adata\n","adata = sc.read_h5ad(results_folder + sample_name+'_adata_final.h5ad')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leTs0A9WuKpe"},"outputs":[],"source":["adata = up_to_pca(adata, n_top_genes=3000, from_raw=True)\n","adata = from_pca_to_leiden(adata, n_neighbors = 20, n_pcs = 15, resolution = 1, plot_spatial=True)\n","# n_top_genes, n_neighbors, n_pcs, resolution potrebbero essere dati come input?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrYq_xwOH8dy"},"outputs":[],"source":["adata.write_h5ad(results_folder + sample_name+'_adata_final.h5ad')   # overwrite"]},{"cell_type":"markdown","metadata":{"id":"GMEkEloZH_m2"},"source":["# 7. NICHE ANALYSIS (PYTHON)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TExJKfWbIFQO"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","import scanpy as sc\n","from sklearn.neighbors import NearestNeighbors\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uuwc2EWiIH9r"},"outputs":[],"source":["# INPUT DATA\n","\n","# adata\n","adata = sc.read_h5ad(results_folder + sample_name+'_adata_final.h5ad')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"it9MmMY9IXJj"},"outputs":[],"source":["# find n nearest neighbours, with max threshold\n","\n","n = 20 # da dare in input il numero di neighbours considerati\n","max_dist = 50 # dare in input max distance (non so in che ordine sia)\n","coordinates = adata.obs[[\"array_row\", \"array_col\"]]\n","neighbors = NearestNeighbors(n_neighbors=n+1, metric='euclidean')\n","neighbors.fit(coordinates)\n","distances, indices = neighbors.kneighbors(coordinates)\n","\n","\n","# Exclude the first column (which is the distance to the point itself)\n","distances = distances[:, 1:]\n","indices = indices[:, 1:]\n","\n","# For each cell, keep only neighbor indices where distance < max_dist\n","filtered_neighbors = []\n","for dist_row, idx_row in zip(distances, indices):\n","    filtered = [idx for d, idx in zip(dist_row, idx_row) if d < max_dist]\n","    filtered_neighbors.append(filtered)\n","\n","neighbors_df = pd.DataFrame(distances, columns=[f'distance_{i+1}' for i in range(n)])\n","neighbors_df['neighbor_indices'] = filtered_neighbors\n","neighbors_df['n_neighbors_within_max_dist'] = neighbors_df['neighbor_indices'].apply(len)\n","\n","adata.obs['n_neighbors_within_max_dist'] = neighbors_df['n_neighbors_within_max_dist']\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GmDm_kUyf4dn"},"outputs":[],"source":["# CELLTYPES distribution among nerighbours\n","\n","neighbor_cell_types_counts = []\n","cell_types = adata.obs.cell_types.unique()\n","col_index = adata.obs.columns.get_loc('cell_types')\n","\n","for i in range(neighbors_df.shape[0]):\n","    idx = np.array(neighbors_df.loc[i, 'neighbor_indices'])\n","    neighbor_cell_types = adata.obs.iloc[idx, col_index]\n","    type_counts = neighbor_cell_types.value_counts()\n","\n","    count_dict = {cell_type: int(type_counts.get(cell_type, 0)) for cell_type in cell_types}\n","    neighbor_cell_types_counts.append(count_dict)\n","\n","neighbor_cell_types_df = pd.DataFrame(neighbor_cell_types_counts, index=adata.obs.index).T # cell types x cells\n","neighbor_cell_types_df = neighbor_cell_types_df.div(neighbor_cell_types_df.sum(axis=0), axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YWgfzNehkQ5l"},"outputs":[],"source":["# KMEANS ON FRACTION OF CELLTYPES IN NEIGHBOURS\n","\n","k = 20  # number of niches, to be given in input\n","\n","X = neighbor_cell_types_df.T  # shape: cells x cell types\n","kmeans = KMeans(n_clusters=k, random_state=42)\n","cluster_labels = kmeans.fit_predict(X)\n","adata.obs['neighbour_niches'] = cluster_labels.astype(str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHmImOp5zWYt"},"outputs":[],"source":["adata.write_h5ad(results_folder + sample_name+'_adata_final.h5ad')   # overwrite"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP4k9EkZuUVqM5LxlvO3c/P","collapsed_sections":["x2hTo2N8Jl5I","R63KcW9rKtC3","tG6sHLJ-tje6"],"provenance":[{"file_id":"106OzTR85p3S-eTWCeOsGvO-sDQdpKnd5","timestamp":1744298048163}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
